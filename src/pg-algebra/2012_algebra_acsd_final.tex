\chapter{Algebra of Parameterised Graphs}

\section{Parameterised Graphs\label{sec:Parametrised-Graphs}}

A \emph{Parameterised Graph} (PG) is a model which has evolved from
Conditional Partial Order Graphs (CPOG)~\cite{2010_mokhov_ieee}.
We consider directed graphs $G=(V,E)$ whose vertices are picked from
the fixed alphabet of \emph{actions} $\mathcal{A}=\{a,b,...\}$. Hence
the vertices of $G$ would usually model actions\emph{ }(or \emph{events}\/)
of the system being designed, while the arcs would usually model the
\emph{precedence} or \emph{causality} relation: if there is an arc
going from~$a$ to~$b$ then action~$a$ precedes action~$b$.
We will denote the \emph{empty graph} $(\emptyset,\emptyset)$ by
$\varepsilon$ and the \emph{singleton graphs} $(\{a\},\emptyset)$
simply by $a$, for any $a\in\mathcal{A}$\emph{.}

Let $G_{1}=(V_{1},E_{1})$ and $G_{2}=(V_{2},E_{2})$ be two graphs,
where $V_{1}$ and $V_{2}$ as well as $E_{1}$ and $E_{2}$ are not
necessarily disjoint. We define the following operations on graphs
(in the order of increasing precedence):
\begin{lyxlist}{00.00.0000}
\item [{\hspace{6mm}Overlay:}] $G_{1}+G_{2}\overset{\mathrm{df}}{=}(V_{1}\cup V_{2},E_{1}\cup E_{2})$.
\item [{\hspace{6mm}Sequence:}] $G_{1}\rightarrow G_{2}\overset{\mathrm{df}}{=}(V_{1}\cup V_{2},E_{1}\cup E_{2}\cup V_{1}\times V_{2})$.
\item [{\hspace{6mm}Condition:}] $[1]G\overset{\mathrm{df}}{=}G$ and
$[0]G\overset{\mathrm{df}}{=}\varepsilon$.
\end{lyxlist}
In other words, the \emph{overlay~}$+$ and \emph{sequence}~$\rightarrow$
are binary operations on graphs with the following semantics: $G_{1}+G_{2}$
is a graph obtained by \emph{overlaying} graphs~$G_{1}$ and~$G_{2}$,
i.e. it contains the union of their vertices and arcs, while graph
$G_{1}\rightarrow G_{2}$ contains the union plus the arcs connecting
every vertex from graph~$G_{1}$ to every vertex from graph~$G_{2}$
(self-loops can be formed in this way if $V_{1}$ and $V_{2}$ are
not disjoint). From the behavioural point of view, if graphs~$G_{1}$
and~$G_{2}$ correspond to two systems then $G_{1}+G_{2}$ corresponds
to their \emph{parallel composition} and $G_{1}\rightarrow G_{2}$
corresponds to their \emph{sequential composition}. One can observe
that any non-empty graph can be obtained by successively applying
the operations $+$ and $\rightarrow$ to the singleton graphs.

Fig.~\ref{fig:Overlay-and-sequence-no-common} shows an example of
two graphs together with their overlay and sequence. One can see that
the overlay does not introduce any dependencies between the actions
coming from different graphs, therefore they can be executed concurrently.
On the other hand, the sequence operation imposes the order on the
actions by introducing new dependencies between actions $a$, $b$
and $c$ coming from graph $G_{1}$ and action $d$ coming from graph
$G_{2}$. Hence, the resulting system behaviour is interpreted as
the behaviour specified by graph $G_{1}$ followed by the behaviour
specified by graph $G_{2}$. Another example of system composition
is shown in Fig.~\ref{fig:Overlay-and-sequence}. Since the graphs
have common vertices, their compositions are more complicated, in
particular, their sequence contains the self-dependencies $(b,b)$
and $(d,d)$ which lead to a \emph{deadlock} in the resulting system:
action $a$ can occur, but all the remaining actions are locked.

Given a graph~$G$, the unary \emph{condition} operations can either
preserve it (\/\emph{true condition} $[1]G$) or nullify it (\/\emph{false
condition} $[0]G$). They should be considered as a family $\{[b]\}_{b\in\mathbb{B}}$
of operations parameterised by a Boolean value~$b$.

Having defined the basic operations on the graphs, one can build graph
expressions using these operations, the empty graph $\varepsilon$,
the singleton graphs $a\in\mathcal{A}$, and the Boolean constants
$0$ and $1$ (as the parameters of the conditional operations) ---
much like the usual arithmetical expressions. We now consider replacing
the Boolean constants with Boolean variables or general predicates
(this step is akin going from arithmetic to algebraic expressions).
The value of such an expression depends on the values of its parameters,
and so we call such an expression a \emph{parameterised graph}~(PG).

One can easily prove the following properties of the operations introduced
above.
\begin{itemize}
\item \hspace{-1mm}Properties of overlay:

\begin{lyxlist}{00.00.0000}
\item [{\hspace{2mm}Identity:}] $G+\varepsilon=G$
\item [{\hspace{2mm}Commutativity:}] $G_{1}+G_{2}=G_{2}+G_{1}$
\item [{\hspace{2mm}Associativity:}] $(G_{1}+G_{2})+G_{3}=G_{1}+(G_{2}+G_{3})$
\end{lyxlist}
\item \hspace{-1mm}Properties of sequence:

\begin{lyxlist}{00.00.0000}
\item [{\hspace{2mm}Left~identity:}] $\varepsilon\rightarrow G=G$
\item [{\hspace{2mm}Right~identity:}] $G\rightarrow\varepsilon=G$
\item [{\hspace{2mm}Associativity:}] $(G_{1}\!\rightarrow\! G_{2})\!\rightarrow\! G_{3}=G_{1}\!\rightarrow\!(G_{2}\!\rightarrow\! G_{3})$
\end{lyxlist}
\item \hspace{-1mm}Other properties:


\hspace{2mm}Left/right~distributivity: \vspace{-0.3em}
\[
\begin{array}{c}
G_{1}\rightarrow(G_{2}+G_{3})=G_{1}\rightarrow G_{2}+G_{1}\rightarrow G_{3}\\
(G_{1}+G_{2})\rightarrow G_{3}=G_{1}\rightarrow G_{3}+G_{2}\rightarrow G_{3}
\end{array}
\]
\vspace{-1em}



\hspace{2mm}Decomposition: \vspace{-0.3em}
\[
G_{1}\!\rightarrow\! G_{2}\!\rightarrow\! G_{3}\!=\! G_{1}\!\rightarrow\! G_{2}+G_{1}\!\rightarrow\! G_{3}+G_{2}\!\rightarrow\! G_{3}
\]
\vspace{-1.3em}


\item \hspace{-1mm}Properties involving conditions:

\begin{lyxlist}{00.00.0000}
\item [{\hspace{2mm}Conditional~$\varepsilon$:}] $[b]\varepsilon=\varepsilon$
\item [{\hspace{2mm}Conditional~overlay:}] $[b](G_{1}+G_{2})=[b]G_{1}+[b]G_{2}$
\item [{\hspace{2mm}Conditional~sequence:}] \hspace{-0.5mm}$[b](G_{1}\!\rightarrow\! G_{2})\!=\![b]G_{1}\!\rightarrow\![b]G_{2}$
\item [{\hspace{2mm}AND-condition:}] $[b_{1}\wedge b_{2}]G=[b_{1}][b_{2}]G$
\item [{\hspace{2mm}OR-condition:}] $[b_{1}\vee b_{2}]G=[b_{1}]G+[b_{2}]G$
\end{lyxlist}

\hspace{2mm}Condition~regularisation: \vspace{-0.3em}
\[
[b_{1}]G_{1}\!\rightarrow\![b_{2}]G_{2}\!=\![b_{1}]G_{1}+[b_{2}]G_{2}+[b_{1}\wedge b_{2}](G_{1}\!\rightarrow\! G_{2})
\]


\end{itemize}
Now, due to the above properties of the operators, it is possible
to define the following canonical form of a PG. In the proof below,
we call a singleton graph, possibly prefixed with a condition, a \emph{literal}.
\begin{prop}
[Canonical form of a PG]\label{prop:Canonical-form} Any PG can be
rewritten in the following canonical form:
\begin{equation}
\left(\sum_{v\in V}[b_{v}]v\right)+\left(\sum_{u,v\in V}[b_{uv}](u\rightarrow v)\right),\label{eq:canonical-form}
\end{equation}


where:
\begin{itemize}
\item $V$ is a subset of singleton graphs that appear in the original PG;
\item for all $v\in V$, $b_{v}$ are canonical forms of Boolean expressions
and are distinct from 0;
\item for all $u,v\in V$, $b_{uv}$ are canonical forms of Boolean expressions
such that $b_{uv}\Rightarrow b_{u}\wedge b_{v}$.
\end{itemize}
\end{prop}
\begin{proof}
(i) First we prove that any PG can be converted to the form~(\ref{eq:canonical-form}).

All the occurrences of $\varepsilon$ in the expression can be eliminated
by the identity and conditional $\varepsilon$ properties (unless
the whole PG equals to $\varepsilon$, in which case we take $V=\emptyset$).
To avoid unconditional subexpressions, we prefix the resulting expression
with `$[1]$', and then by the conditional overlay/sequence properties
we propagate all the conditions that appear in the expression down
to the singleton graphs (compound conditions can be always reduced
to a single one by the AND-condition property). By the decomposition
and distributivity properties, the expression can be rewritten as
an overlay of literals and subexpressions of the form $l_{1}\rightarrow l_{2}$,
where $l_{1}$ and $l_{2}$ are literals. The latter subexpressions
can be rewritten using the condition regularisation rule:
\[
[b_{1}]u\rightarrow[b_{2}]v=[b_{1}]u+[b_{2}]v+[b_{1}\wedge b_{2}](u\rightarrow v)
\]
Now, literals corresponding to the same singleton graphs, as well
as subexpressions of the form $[b](u\rightarrow v)$ that correspond
to the same pair of singleton graphs $u$ and $v$, are combined using
the OR-condition property. Then the literals prefixed with 0 conditions
can be dropped. Now the set $V$ consists of all the singleton graphs
occurring in the literals. To turn the overall expression into the
required form it only remains to add missing subexpressions of the
form $[0](u\rightarrow v)$ for every $u,v\in V$ such that the expression
does not contain the subexpression of the form $[b](u\rightarrow v)$.
Note that the property $b_{uv}\Rightarrow b_{u}\wedge b_{v}$ is always
enforced by this construction:
\begin{itemize}
\item condition regularisation ensures this property;
\item combining literals using the OR-condition property can only strengthen
the right hand side of this implication, and so cannot violate it;
\item adding $[0](u\rightarrow v)$ does not violate the property as it
trivially holds when $b_{uv}=0$.
\end{itemize}
(ii) We now show that (\ref{eq:canonical-form}) is a canonical form,
i.e. if $L=R$ then their canonical forms $\mathit{can}(L)$ and $\mathit{can}(R)$
coincide.

For the sake of contradiction, assume this is not the case. Then we
consider two cases (all possible cases are symmetric to one of these
two):
\begin{enumerate}
\item $\mathit{can}(L)$ contains a literal $[b_{v}]v$ whereas $\mathit{can}(R)$
either contains a literal $[b_{v}']v$ with $b_{v}'\not\equiv b_{v}$
or does not contain any literal corresponding to $v$, in which case
we say that it contains a literal $[b_{v}']v$ with $b_{v}'=0$. Then
for some values of parameters one of the graphs will contain vertex
$v$ while the other will not.
\item $\mathit{can}(L)$ and $\mathit{can}(R)$ have the same set $V$ of
vertices, but $\mathit{can}(L)$ contains a subexpression \foreignlanguage{english}{$[b_{uv}](u\rightarrow v)$}
whereas $\mathit{can}(R)$ contains a subexpression \foreignlanguage{english}{$[b_{uv}'](u\rightarrow v)$}
with $b_{uv}'\not\equiv b_{uv}$. Then for some values of parameters
one of the graphs will contain the arc $(u,v)$ (note that due to
$b_{uv}\Rightarrow b_{u}\wedge b_{v}$ and $b_{uv}'\Rightarrow b_{u}\wedge b_{v}$
vertices $u$ and $v$ are present), while the other will not.
\end{enumerate}
In both cases there is a contradiction with $L=R$.
\end{proof}
This canonical form allows one to lift the notion of \emph{adjacency
matrix} of a graph to PGs. Recall that the adjacency matrix $(b_{uv})$
of a graph $(V,E)$ is a $|V|\times|V|$ Boolean matrix such that
$b_{uv}=1$ if $(u,v)\in E$ and $b_{uv}=0$ otherwise. The adjacency
matrix of a PG is obtained from the canonical form~(\ref{eq:canonical-form})
by gathering the predicates $b_{uv}$ into a matrix. The adjacency
matrix of a PG is similar to that of a graph, but it contains predicates
rather than Boolean values. It does not uniquely determine a PG, as
the predicates of the vertices cannot be derived from it; to fully
specify a PG one also has to provide predicates $b_{v}$ from the
canonical form~(\ref{eq:canonical-form}). 

Another advantage of this canonical form is that it provides a graphical
notation for PGs. The vertices occurring in the canonical form (set
$V$) can be represented by circles, and the subexpressions of the
form $u\rightarrow v$ by arcs. The label of a vertex $v$ consists
of the vertex name, colon and the predicate $b_{v}$, while every
arc~$(u,v)$ is labelled with the corresponding predicate $b_{uv}$.
As adjacency matrices of PGs tend to have many constant elements,
we use a simplified notation in which the arcs with constant~0 predicates
are not drawn, and constant~1 predicates are dropped; moreover, it
is convenient to assume that the predicates on arcs are implicitly
ANDed with those on incident vertices (to enforce the invariant $b_{uv}\Rightarrow b_{u}\wedge b_{v}$),
which often allows one to simplify predicates on arcs. This can be
justified by introducing the ternary operator, called \emph{conditional
sequence}:
\[
u\overset{b}{\longrightarrow}v\overset{\text{df}}{=}[b](u\rightarrow v)+u+v
\]
Intuitively, PG $u\overset{b}{\longrightarrow}v$ consists of two
unconditional vertices connected by an arc with the condition $b$.
By case analysis on $b_{1}$ and $b_{2}$ one can easily prove the
following properties of the conditional sequence that allow simplifying
the predicates on arcs:
\begin{eqnarray*}
[b_{1}]u\xrightarrow{b_{1}\wedge b_{2}}v & = & [b_{1}]u\overset{b_{2}}{\longrightarrow}v\\
u\xrightarrow{b_{1}\wedge b_{2}}[b_{2}]v & = & u\overset{b_{1}}{\longrightarrow}[b_{2}]v
\end{eqnarray*}


Fig.~\ref{fig:Specialisations}(top) shows an example of a PG. The
predicates depend on a Boolean variable~$x$. The predicates of vertices~$a$,
$b$ and $d$ are constants~$1$; such vertices are called \emph{unconditional}.
Vertices~$c$ and~$e$ are \emph{conditional,} and their predicates
are $x$ and $\overline{x}$, respectively. Arcs also fall into two
classes: \emph{unconditional,} i.e. those whose predicate and the
predicates of their incident vertices are constants~1, and \emph{conditional}
(in this example, all the arcs are conditional).

A \emph{specialisation $H\vert_{p}$ of a PG $H$ under predicate
$p$} is a PG, whose predicates are simplified under the assumption
that $p$ holds. If $H$ specifies the behaviour of the whole system,
$H\vert_{p}$ specifies the part of the behaviour that can be realised
under condition $p$. An example of a graph and its two specialisations
is presented in Fig.~\ref{fig:Specialisations}. The leftmost specialisation
$H\vert_{x}$ is obtained by removing from the graph those vertices
and arcs whose predicates evaluate to~0 under condition~$x$, and
simplifying the other predicates. Hence, vertex~$e$ and arcs $(a,d)$,
$(a,e)$, $(b,d)$ and $(b,e)$ disappear, and all the other vertices
and arcs become unconditional. The rightmost specialisation $H\vert_{\overline{x}}$
is obtained analogously. Each of the obtained specialisations can
be regarded as a specification of a particular behavioural scenario
of the modelled system, e.g. as specification of a processor instruction.

\begin{figure}
\begin{centering}
\includegraphics[width=1\columnwidth]{fig/cpog_projections_2}
\par\end{centering}

\caption{\label{fig:Specialisations}PG specialisations: $H\vert_{x}$ and
$H\vert_{\overline{x}}$}
\end{figure}



\subsection{Specification and composition of instructions}

Consider a processing unit that has two registers $A$ and $B$, and
can perform two different instructions: \emph{addition} and \emph{exchange}
of two variables stored in memory. The processor contains five datapath
components (denoted by $a\dots e$) that can perform the following
atomic actions:\renewcommand{\labelenumi}{\alph{enumi})}
\begin{enumerate}
\item Load register $A$ from memory;
\item Load register $B$ from memory;
\item Compute the sum of the numbers stored in registers~$A$ and~$B$,
and store it in $A$;
\item Save register $A$ into memory;
\item Save register $B$ into memory.
\end{enumerate}
\renewcommand{\labelenumi}{\arabic{enumi}.}Table~\ref{tab-two-operations}
describes the addition and exchange instructions in terms of usage
of these atomic actions.

The addition instruction consists of loading the two operands from
memory (causally independent actions~$a$ and~$b$), their addition
(action~$c$), and saving the result (action~$d$). Let us assume
for simplicity that in this example all causally independent actions
are always performed concurrently, see the corresponding scenario
$\mathit{ADD}$ in the table.

\begin{table}
\begin{centering}
\begin{tabular}{|c||c||c|c|}
\hline 
\multicolumn{2}{|c||}{Instruction} & Addition & Exchange\tabularnewline
\hline 
\multicolumn{2}{|c||}{} & \multicolumn{1}{l|}{~a) Load $A$} & \multicolumn{1}{l|}{~a) Load $A$}\tabularnewline
\multicolumn{2}{|c||}{Action} & \multicolumn{1}{l|}{~b) Load $B$} & \multicolumn{1}{l|}{~b) Load $B$}\tabularnewline
\multicolumn{2}{|c||}{sequence} & \multicolumn{1}{l|}{~c) Add $B$ to $A$} & \multicolumn{1}{l|}{~d) Save $A$}\tabularnewline
\multicolumn{2}{|c||}{} & \multicolumn{1}{l|}{~d) Save $A$} & \multicolumn{1}{l|}{~e) Save $B$}\tabularnewline
\hline 
\multicolumn{2}{|c||}{Execution} & \includegraphics[bb=-10bp 90bp 158bp 220bp,scale=0.4]{fig/projection_1}~~~ & \includegraphics[bb=-20bp 90bp 157bp 220bp,scale=0.4]{fig/projection_2}\tabularnewline
\multicolumn{2}{|c||}{scenario} &  & \tabularnewline
\multicolumn{2}{|c||}{with maximum} &  & \tabularnewline
\multicolumn{2}{|c||}{concurrency} &  & \tabularnewline
\multicolumn{2}{|c||}{} &  & \tabularnewline
\multicolumn{2}{|c||}{} &  & \tabularnewline
\multicolumn{2}{|c||}{} & $\mathit{ADD}$ & $\mathit{XCHG}$\tabularnewline
\multicolumn{2}{|c||}{} &  & \tabularnewline
\hline 
\end{tabular}
\par\end{centering}

\caption{\label{tab-two-operations}Two instructions specified as partial orders}
\end{table}


The operation of exchange consists of loading the operands (causally
independent actions~$a$ and~$b$), and saving them into swapped
memory locations (causally independent actions $d$ and $e$), as
captured by the $\mathit{XCHG}$ scenario. Note that in order to start
saving one of the registers it is necessary to wait until both of
them have been loaded to avoid overwriting one of the values.

One can see that the two scenarios in Table~\ref{tab-two-operations}
appear to be the two specialisations of the PG shown in Fig.~\ref{fig:Specialisations},
thus this PG can be considered as a joint specification of both instructions.
Two important characteristics of such a specification are that the
common events $\{a,b,d\}$ are overlaid, and the choice between the
two operations is modelled by the Boolean predicates associated with
the vertices and arcs of the PG. As a result, in our model there is
no need for a `nodal point' of choice, which tend to appear in alternative
specification models: a Petri Net (resp. Finite State Machine) would
have an explicit choice place (resp. state), and a specification written
in a Hardware Description Language would describe the two instructions
by two separate branches of a conditional statement~\texttt{if} or~\texttt{case}~\cite{1994_de_micheli_book}).

The PG operations introduced above allow for a natural specification
of the system as a collection of its behavioural scenarios, which
can share some common parts. For example, in this case the overall
system is composed as
\begin{equation}
\begin{array}{c}
H=[x]ADD+[\overline{x}]XCHG=\\
=\![x]((a\!+\! b)\!\rightarrow\! c\!+\! c\!\rightarrow\! d)\!+\![\overline{x}]((a\!+\! b)\!\rightarrow\!(d\!+\! e)).
\end{array}\label{eq:H_ADD_XCHG}
\end{equation}
Such specifications can often be simplified using the properties of
graph operations. The next section describes the equivalence relation
between the PGs with a set of axioms, thus obtaining an algebra.


\section{Algebra of parameterised graphs\label{sec:Algebra-of-parametrised}}

In this section we define the \emph{algebra of parameterised graphs}
(PG-algebra).

PG-algebra is a tuple~$\left\langle \mathcal{G},+,\rightarrow,[0],[1]\right\rangle $,
where~$\mathcal{G}$ is a set of graphs whose vertices are picked
from the alphabet~$\mathcal{A}$ and the operations parallel those
defined for graphs above. The equivalence relation is given by the
following axioms.
\begin{itemize}
\item $+$ is commutative and associative
\item $\rightarrow$ is associative
\item $\varepsilon$ is a left and right identity of $\rightarrow$
\item $\rightarrow$ distributes over $+$:\vspace{-0.3em}
\[
\begin{array}{c}
p\rightarrow(q+r)=p\rightarrow q+p\rightarrow r\\
(p+q)\rightarrow r=p\rightarrow r+q\rightarrow r
\end{array}
\]

\item Decomposition: \vspace{-0.3em}
\[
p\rightarrow q\rightarrow r=p\rightarrow q+p\rightarrow r+q\rightarrow r
\]

\item Condition: $[0]p=\varepsilon$ and $[1]p=p$
\end{itemize}
The following derived equalities can be proved from PG-algebra axioms~\cite[Prop. 2, 3]{2011_mokhov_pg}:
\begin{itemize}
\item $\varepsilon$ is an identity of $+$: $p+\varepsilon=p$
\item $+$ is idempotent: $p+p=p$
\item Left and right absorption:\vspace{-0.3em}
\[
\begin{array}{c}
p+p\rightarrow q=p\rightarrow q\\
q+p\rightarrow q=p\rightarrow q
\end{array}
\]

\item Conditional $\varepsilon$: $[b]\varepsilon=\varepsilon$
\item Conditional overlay: $[b](p+q)=[b]p+[b]q$
\item Conditional sequence: $[b](p\rightarrow q)=[b]p\rightarrow[b]q$
\item AND-condition: $[b_{1}\wedge b_{2}]p=[b_{1}][b_{2}]p$
\item OR-condition: $[b_{1}\vee b_{2}]p=[b_{1}]p+[b_{2}]p$
\item Choice propagation:\vspace{-0.3em}
\[
\begin{array}{c}
[b](p\rightarrow q)+[\overline{b}](p\rightarrow r)=p\rightarrow([b]q+[\overline{b}]r)\\
{}[b](p\rightarrow r)+[\overline{b}](q\rightarrow r)=([b]p+[\overline{b}]q)\rightarrow r
\end{array}
\]

\item Condition regularisation:\vspace{-0.3em}
\[
[b_{1}]p\rightarrow[b_{2}]q=[b_{1}]p+[b_{2}]q+[b_{1}\wedge b_{2}](p\rightarrow q)
\]

\end{itemize}
Note that as $\varepsilon$ is a left and right identity of $\rightarrow$
and $+$, there can be no other identities for these operations. Interestingly,
unlike many other algebras, the two main operations in the PG-algebra
have the same identity.

It is easy to see that PGs are a model of PG-algebra, as all the axioms
of PG-algebra are satisfied by PGs; in particular, this means that
PG-algebra is \emph{sound}. Moreover, any PG-algebra expression has
the canonical form~(\ref{eq:canonical-form}), as the proof of Prop.~\ref{prop:Canonical-form}
can be directly imported: 
\begin{itemize}
\item It is always possible to translate a PG-algebra expression to this
canonical form, as part~(i) of the proof relies only on the properties
of PGs that correspond to either PG-algebra axioms or equalities above.
\item If $L=R$ holds in PG-algebra then $L=R$ holds also for PGs (as PGs
are a model of PG-algebra), and so the PGs $\mathit{can}(L)$ and
$\mathit{can}(R)$ coincide, see part~(ii) of the proof. Since PGs
$\mathit{can}(L)$ and $\mathit{can}(R)$ are in fact the same objects
as the expressions $\mathit{can}(L)$ and $\mathit{can}(R)$ of the
PG-algebra, (\ref{eq:canonical-form}) is a canonical form of a PG-algebra
expression.
\end{itemize}
This also means that PG-algebra is \emph{complete} w.r.t. PGs, i.e.
any PG equality can be either proved or disproved using the axioms
of PG-algebra (by converting to the canonical form). 

The provided set of axioms of PG-algebra is \emph{minimal}, i.e. no
axiom from this set can be derived from the others. The minimality
was checked by enumerating the fixed-size models of PG-algebra with
the help of the \noun{Alg} tool~\cite{2011_bizjak_alg}: It turns
out that removing any of the axioms leads to a different number of
non-isomorphic models of a particular size, implying that all the
axioms are necessary.

Hence, the following result holds:
\begin{thm}
[Soundness, Minimality and Completeness] The set of axioms of PG-algebra
is sound, minimal and complete w.r.t. PGs.
\end{thm}
\begin{figure*}
\begin{eqnarray*}
[x]((a+b)\rightarrow c+c\rightarrow d)+[\overline{x}]((a+b)\rightarrow(d+e)) & = & (\textrm{closure)}\\
{}[x]((a+b)\rightarrow c+(a+b)\rightarrow d+c\rightarrow d)+[\overline{x}]((a+b)\rightarrow(d+e)) & = & (\textrm{decomposition)}\\
{}[x]((a+b)\rightarrow c\rightarrow d)+[\overline{x}]((a+b)\rightarrow(d+e)) & = & (\textrm{choice propagation)}\\
(a+b)\rightarrow([x](c\rightarrow d)+[\overline{x}](d+e)) & = & (\textrm{conditional overlay)}\\
(a+b)\rightarrow([x](c\rightarrow d)+[\overline{x}]d+[\overline{x}]e) & = & (\rightarrow-\textrm{identity)}\\
(a+b)\rightarrow([x](c\rightarrow d)+[\overline{x}](\varepsilon\rightarrow d)+[\overline{x}]e) & = & (\textrm{choice propagation)}\\
(a+b)\rightarrow(([x]c+[\overline{x}]\varepsilon)\rightarrow d+[\overline{x}]e) & = & (\textrm{conditional \ensuremath{\varepsilon}, +-\textrm{identity)}}\\
(a+b)\rightarrow([x]c\rightarrow d+[\overline{x}]e).
\end{eqnarray*}


\caption{Simplifying expression~(\ref{eq:H_ADD_XCHG}) using the Closure axiom\label{fig:Simplifying-TPG-expressions}}
\end{figure*}



\section{Transitive parameterised graphs and their algebra}

In many cases the arcs of the graphs are interpreted as the causality
relation, and so the graph itself is a partial order. However, in
practice it is convenient to drop some or all of the transitive arcs,
i.e. two graphs should be considered equal whenever their transitive
closures are equal. E.g. in this case the graphs specified by the
expressions $a\rightarrow b+b\rightarrow c$ and $a\rightarrow b+a\rightarrow c+b\rightarrow c$
are considered as equal. PGs with this equality relation are called
\emph{Transitive Parameterised Graphs} (TPG). To capture this algebraically,
we augment the PG-algebra with the \emph{Closure} axiom:
\[
\mbox{{if\ }}q\neq\varepsilon\mbox{{\ then\ }}p\!\rightarrow\! q+q\!\rightarrow\! r=p\!\rightarrow\! q+p\!\rightarrow\! r+q\!\rightarrow\! r.
\]
One can see that by repeated application of this axiom one can obtain
the transitive closure of any graph, including those with cycles.
The resulting algebra is called Transitive Parameterised Graphs Algebra
(TPG-algebra).

Note that the condition $q\ne\varepsilon$ in the Closure axiom is
necessary, as otherwise
\[
a+b=a\!\rightarrow\!\varepsilon+\varepsilon\!\rightarrow\! b=a\!\rightarrow\!\varepsilon+a\!\rightarrow\! b+\varepsilon\!\rightarrow\! b=a\!\rightarrow\! b,
\]
and the operations $+$ and $\rightarrow$ become identical, which
is clearly undesirable.

The Closure axiom helps to simplify specifications by reducing the
number of arcs and/or simplifying their conditions. For example, consider
the PG expression~(\ref{eq:H_ADD_XCHG}). As the scenarios of this
PG are interpreted as the orders of execution of actions, it is natural
to use the Closure axiom. Note that the expression cannot be simplified
in PG-algebra; however, in the TPG-algebra it can be considerably
simplified, as shown in Fig.~\ref{fig:Simplifying-TPG-expressions}.

The corresponding TPG is shown in Fig.~\ref{fig:The-simplified-CG-from}.
Note that it has fewer conditional elements than the PG in Fig.~\ref{fig:Specialisations};
though the specialisations are now different, they have the same transitive
closures.

We now lift the canonical form~(\ref{eq:canonical-form}) to TPGs
and TPG-algebra. Note that the only difference is the last requirement.
\begin{prop}
[Canonical form of a TPG]\label{prop:Canonical-form-tpg} Any TPG
can be rewritten in the following canonical form:
\begin{equation}
\left(\sum_{v\in V}[b_{v}]v\right)+\left(\sum_{u,v\in V}[b_{uv}](u\rightarrow v)\right),\label{eq:canonical-form-tpg}
\end{equation}
 where:
\begin{enumerate}
\item $V$ is a subset of singleton graphs that appear in the original TPG;
\item for all $v\in V$, $b_{v}$ are canonical forms of Boolean expressions
and are distinct from 0;
\item for all $u,v\in V$, $b_{uv}$ are canonical forms of Boolean expressions
such that $b_{uv}\Rightarrow b_{u}\wedge b_{v}$;
\item for all $u,v,w\in V$, $b_{uv}\wedge b_{vw}\Rightarrow b_{uw}$.
\end{enumerate}
\end{prop}
\begin{proof}
(i) First we prove that any TPG can be converted to the form~(\ref{eq:canonical-form-tpg}).

We can convert the expression into the canonical form (\ref{eq:canonical-form}),
which satisfies the requirements 1--3. Then we iteratively apply the
following transformation, while possible: If for some $u,v,w\in V$,
$b_{uv}\wedge b_{vw}\Rightarrow b_{uw}$ does not hold (i.e. requirement
4 is violated), we replace the subexpression $[b_{uw}](u\rightarrow w)$
with $[b_{uw}^{\mathit{new}}](u\rightarrow w)$ where $b_{uw}^{\mathit{new}}\overset{\text{df}}{=}b_{uw}\vee(b_{uv}\wedge b_{vw})$.
Observe that after this the requirement 4 will hold for $u$, $v$
and $w$, and the requirement 3 remains satisfied, i.e. $b_{uw}^{\mathit{new}}\Rightarrow b_{u}\wedge b_{w}$
due to $b_{uv}\Rightarrow b_{u}\wedge b_{v}$, $b_{vw}\Rightarrow b_{v}\wedge b_{w}$
and $b_{uw}\Rightarrow b_{u}\wedge b_{w}$. Moreover, the resulting
expression will be equivalent to the one before this transformation
due to the following equality (see~\cite{2011_mokhov_pg} for the
proof):
\[
\begin{array}{c}
\mbox{{If\ }}v\neq\varepsilon\mbox{{\ then\ }}[b_{uv}](u\rightarrow v)+[b_{vw}](v\rightarrow w)=\\
=[b_{uv}](u\rightarrow v)+[b_{vw}](v\rightarrow w)+[b_{uv}\wedge b_{vw}](u\rightarrow w).
\end{array}
\]


This iterative process converges, as there can be only finitely many
expressions of the form (\ref{eq:canonical-form-tpg}) (recall that
we assume that the predicates within the conditional operators are
always in some canonical form), and each iteration replaces some predicate
$b_{uw}$ with a greater one $b_{uw}^{\mathit{new}}$, in the sense
that $b_{uv}$ strictly subsumes $b_{uw}^{\mathit{new}}$ (i.e. $b_{uw}\Rightarrow b_{uw}^{\mathit{new}}$
and $b_{uw}\not\equiv b_{uw}^{\mathit{new}}$ always hold), i.e. no
predicate can be repeated during these iterations.

(ii) We now show that (\ref{eq:canonical-form-tpg}) is a canonical
form, i.e. if $L=R$ then their canonical forms $\mathit{can}(L)$
and $\mathit{can}(R)$ coincide.

For the sake of contradiction, assume this is not the case. Then we
consider two cases (all possible cases are symmetric to one of these
two).
\begin{enumerate}
\item $\mathit{can}(L)$ contains a literal $[b_{v}]v$ whereas $\mathit{can}(R)$
either contains a literal $[b_{v}']v$ with $b_{v}'\neq b_{v}$ or
does not contain any literal corresponding to $v$, in which case
we say that it contains a literal $[b_{v}']v$ with $b_{v}'=0$. Then
for some values of parameters one of the graphs will contain vertex
$v$ while the other will not.
\item $\mathit{can}(L)$ and $\mathit{can}(R)$ have the same set $V$ of
vertices, but $\mathit{can}(L)$ contains a subexpression \foreignlanguage{english}{$[b_{uv}](u\!\rightarrow\! v)$}
and $\mathit{can}(R)$ contains a subexpression \foreignlanguage{english}{$[b_{uv}'](u\!\rightarrow\! v)$}
with $b_{uv}'\not\equiv b_{uv}$. Then for some values of parameters
one of the graphs will contain the arc $(u,v)$ while the other will
not. Since the transitive closures of the graphs must be the same
due to $\mathit{can}(L)\!=\! L\!=\! R\!=\!\mathit{can}(R)$, the other
graph must contain a path $t_{1}t_{2}\ldots t_{n}$ where $u\!=\! t_{1}$,
$v\!=\! t_{n}$ and $n\!\geq\!3$; w.l.o.g., we assume that $t_{1}t_{2}\ldots t_{n}$
is a shortest such path. Hence, the canonical form (\ref{eq:canonical-form})
would contain the subexpressions $[b_{t_{i}t_{i+1}}](t_{i}\!\rightarrow\! t_{i+1})$,
$i=1\ldots n-1$, and moreover $\bigwedge_{i=1}^{n-1}b_{t_{i}t_{i+1}}\neq0$
for the chosen values of the parameters, and so $\bigwedge_{i=1}^{n-1}b_{t_{i}t_{i+1}}\not\equiv0$.
But then the iterative process above would have added to the canonical
form the missing subexpression $[b_{t_{1}t_{2}}\wedge b_{t_{2}t_{3}}](t_{1}\!\rightarrow\! t_{3})$,
as the corresponding predicates $\not\equiv0$. Hence, for the chosen
values of the parameters, there is an arc $(t_{1},t_{3})$, contradicting
the assumption that $t_{1}t_{2}\ldots t_{n}$ is a shortest path between~$u$
and~$v$.
\end{enumerate}
In both cases there is a contradiction with $L=R$.
\end{proof}
The process of constructing the canonical form~(\ref{eq:canonical-form-tpg})
of a TPG from the canonical form~(\ref{eq:canonical-form}) of a
PG corresponds to computing the transitive closure of the adjacency
matrix. As the entries of this matrix are predicates rather than Boolean
values, this has to be done symbolically. This is always possible,
as each entry of the resulting matrix can be represented as a finite
Boolean expression depending on the entries of the original matrix
only.

\begin{figure}
\begin{centering}
\includegraphics[width=1\columnwidth]{\string"fig/cpog_projections_2 _trans\string".eps}
\par\end{centering}

\caption{The PG from Fig.~\ref{fig:Specialisations} simplified using the
Closure axiom, together with its specialisations\label{fig:The-simplified-CG-from}}
\vspace{-6mm}
\end{figure}


By the same reasoning as in the previous section, we can conclude
that the following result holds.
\begin{thm}
[Soundness, Minimality and Completeness] The set of axioms of TPG-algebra
is sound, minimal and complete w.r.t. TPGs.
\end{thm}

\section{Case studies}

In this section we consider several practical case studies from hardware
synthesis. The advantage of (T)PG-algebra is that it allows for a
formal and compositional approach to system design. Moreover, using
the rules of (T)PG-algebra one can formally manipulate specifications,
in particular, algebraically simplify them.


\subsection{Phase encoders}

This section demonstrates the application of PG-algebra to designing
the \emph{multiple rail phase encoding} controllers\emph{~}\cite{2006_cdalessandro_async}.
They use several wires for communication, and data is encoded by the
order of occurrence of transitions in the communication lines. Fig.~\ref{fig:phase-encoding}(a)
shows an example of a data packet transmission over a 4-wire phase
encoding communication channel. The order of rising signals on wires
indicates that permutation $abdc$ is being transmitted. In total
it is possible to transmit any of the $n!$ different permutations
over an $n$-wire channel in one communication cycle. This makes the
multiple rail phase encoding protocol very attractive for its information
efficiency~\cite{2010_mokhov_ieee}. 

\begin{figure}
\begin{centering}
\hfill{}\subfloat[Phase encoded data]{\begin{centering}
\includegraphics[scale=0.63]{fig/packet}
\par\end{centering}

}\hfill{}\subfloat[Matrix phase encoder]{\includegraphics[scale=0.41]{fig/matrix_phase_encoder}



}\hfill{}
\par\end{centering}

\caption{Multiple rail phase encoding\label{fig:phase-encoding}}
\vspace{-6mm}
\end{figure}


Phase encoding controllers contain an exponential number of behavioural
scenarios w.r.t. the number of wires, and are very difficult for specification
and synthesis using conventional approaches. In this section we apply
PG-algebra to specification of an $n$-wire \emph{matrix phase encoder}
-- a basic phase encoding controller that generates a permutation
of signal events given a matrix representing the order of the events
in the permutation.

Fig.~\ref{fig:phase-encoding}(b) shows the top-level view of the
controller's structure. Its inputs are ${n \choose 2}$ dual-rail
ports that specify the order of signals to be produced at the controller's
$n$ output wires. The inputs of the controller can be viewed as an
$n\times n$ Boolean matrix $(x_{ij})$ with diagonal elements being
0. The outputs of the controller will be modelled by $n$ actions
$v_{i}\in\mathcal{A}$. Whenever $x_{ij}=1$, event~$v_{i}$ must
happen before event~$v_{j}$. It is guaranteed that $x_{ij}$ and
$x_{ji}$ cannot be 1 at the same time, however, they can be simultaneously
0, meaning that the relative order of the events is not known yet
and the controller has to wait until $x_{ij}=1$ or $x_{ji}=1$ is
satisfied (other outputs for which the order is already known can
be generated meanwhile). 

The overall specification of the controller is obtained as the overlay
${\displaystyle \sum_{1\le i<j\le n}}H_{ij}$ of fixed-size expressions
$H_{ij}$, modelling the behaviour of each pair of outputs. In turn,
each $H_{ij}$ is an overlay of three possible scenarios:
\begin{enumerate}
\item If $x_{ij}=1$ (and so $x_{ji}=0$) then there is a causal dependency
between $v_{i}$ and $v_{j}$, described using the PG-algebra sequence
operator: $v_{i}\rightarrow v_{j}$. 
\item If $x_{ji}=1$ (and so $x_{ij}=0$) then there is a causal dependency
between $v_{j}$ and $v_{i}$: $v_{j}\rightarrow v_{i}$. 
\item If $x_{ij}=x_{ji}=0$ then neither $v_{i}$ nor $v_{j}$ can be produced
yet; this is expressed by a circular wait condition between $v_{i}$
and $v_{j}$: $v_{i}\rightarrow v_{j}+v_{j}\rightarrow v_{i}$.%
\footnote{There are other ways to describe this scenario, e.g. by creating self-loops
$v_{i}\rightarrow v_{i}+v_{j}\rightarrow v_{j}$.%
} 
\end{enumerate}
We prefix each of the scenarios with its precondition and overlay
the results:
\[
\begin{array}{c}
H_{ij}=[x_{ij}\wedge\overline{x_{ji}}](v_{i}\rightarrow v_{j})+[x_{ji}\wedge\overline{x_{ij}}](v_{j}\rightarrow v_{i})+\\
+[\overline{x_{ij}}\wedge\overline{x_{ji}}](v_{i}\rightarrow v_{j}+v_{j}\rightarrow v_{i}).
\end{array}
\]
Using the rules of PG-algebra, we can simplify this expression to
\[
[\overline{x_{ji}}](v_{i}\rightarrow v_{j})+[\overline{x_{ij}}](v_{j}\rightarrow v_{i}),
\]
or, using the conditional sequence operator, to
\[
[\overline{x_{ij}}\vee\overline{x_{ji}}](v_{i}\overset{\overline{x_{ji}}}{\longrightarrow}v_{j}+v_{j}\overset{\overline{x_{ij}}}{\longrightarrow}v_{i}).
\]


Now, bearing in mind that condition $[\overline{x_{ij}}\vee\overline{x_{ji}}]$
is assumed to hold in the proper controller environment ($x_{ij}$
and $x_{ji}$ cannot be 1 simultaneously), we can replace it with
$[1]$ and drop it. The resulting expression can be graphically represented
as shown in Fig.~\ref{fig:CGs-related-to}(a). An example of an overall
controller specification ${\displaystyle \sum_{1\le i<j\le n}}H_{ij}$
for the case when $n=3$ is shown in Fig.~\ref{fig:CGs-related-to}(b).
The synthesis of this specification to a digital circuit can be performed
in a way similar to~\cite{2010_mokhov_ieee}.

\begin{figure}
\hfill{}\subfloat[$H_{ij}$]{

\includegraphics[scale=0.5]{fig/cpog_matrix_sender_ij}}\hfill{}\subfloat[$H_{12}+H_{13}+H_{23}$]{

\includegraphics[scale=0.5]{fig/cpog_matrix_sender}}\hfill{}

\caption{PGs related to matrix phase encoder specification\label{fig:CGs-related-to}}
\vspace{-6mm}
\end{figure}



\subsection{Processor microcontroller and instruction set design}

This section demonstrates application of TPG-algebra to designing
processor microcontrollers. Specification of such a complex system
as a processor has to start at the architectural level, which helps
to manage the system complexity by structural abstraction~\cite{1994_de_micheli_book}.

Fig.~\ref{app-fig-Architecture-of-example} shows the architecture
of an example processor. Separate \emph{Program memory} and \emph{Data
memory} blocks are accessed via the \emph{Instruction fetch} (IFU)
and \emph{Memory access} (MAU) units, respectively. The other two
operational units are: \emph{Arithmetic logic unit} (ALU) and \emph{Program
counter increment unit} (PCIU). The units are controlled using request-acknowledgement
interfaces (depicted as bidirectional arrows) by the\textbf{\emph{
}}\emph{Central microcontroller}, which is our primary design objective. 

The processor has four registers: two general purpose registers $A$
and $B$, \emph{Program counter} (PC) storing the address of the current
instruction in the program memory, and the \emph{Instruction register}
(IR) storing the \emph{opcode} (operation code) of the current instruction.
For the purpose of this paper, the actual width of the registers (the
number of bits they can store) is not important. ALU has access to
all the registers via the register bus; MAU has access to general
purpose registers only; IFU, given the address of the next instruction
in PC, reads its opcode into IR; and PCIU is responsible for incrementing
PC (moving to the next instruction). The microcontroller has access
to the IR and ALU \emph{flags} (information about the current state
of ALU which is used in branching instructions).

Now we define the set of instructions of the processor. Rather than
listing all the instructions, we describe classes of instructions
with the same \emph{addressing mode}~\cite{mspmanual} and the same
execution scenario. As the scenarios here are partial orders of actions,
we use TPG-algebra, and the corresponding TPGs are shown in Fig.~\ref{app-fig-Scenarios-of-8}.

\textbf{ALU operation Rn to Rn}\quad{}An instruction from this class
takes two operands stored in the general purpose registers ($A$ and
$B$), performs an operation, and writes the result back into one
of the registers (so called \emph{register direct addressing mode}).
Examples: $\mathit{ADD\ A,\ B}$ -- addition $A:=A+B$; $\mathit{MOV\ B,\ A}$
-- assignment $B:=A$. ALU works concurrently with PCIU and IFU, which
is captured by the expression $\mathit{ALU}+\mathit{PCIU\rightarrow\mathit{IFU}}$;
the corresponding PG is shown in Fig.~\ref{app-fig-Scenarios-of-8}(a).
As soon as both concurrent branches are completed, the processor is
ready to execute the next instruction. Note that it is not important
for the microcontroller which particular ALU operation is being executed
($\mathit{ADD}$, $\mathit{MOV}$, or any other instruction from this
class) because the scenario is the same from its point of view (it
is the responsibility of ALU to detect which operation it has to perform
according to the current opcode). 

\begin{figure}
\begin{centering}
\includegraphics[width=1.04\columnwidth]{fig/processor_architecture}
\par\end{centering}

\caption{Architecture of an example processor\label{app-fig-Architecture-of-example}}
\vspace{-6mm}
\end{figure}


\begin{figure*}
\begin{centering}
\subfloat[ALU op. Rn to Rn]{

\includegraphics[scale=0.36]{fig/po_ALU_Rn_Rn}}\hfill{}\subfloat[ALU op. \#123 to Rn]{

\includegraphics[scale=0.36]{fig/po_ALU_123_Rn}}\hfill{}\subfloat[ALU op. Rn to PC]{

\includegraphics[scale=0.36]{fig/po_ALU_Rn_PC}}\hfill{}\subfloat[ALU op. \#123 to PC]{

\includegraphics[scale=0.36]{fig/po_ALU_123_PC}}
\par\end{centering}

\begin{centering}
\subfloat[Memory access]{

\includegraphics[scale=0.36]{fig/po_MAU}}\hfill{}\subfloat[Cond. ALU op. Rn to Rn]{

\includegraphics[scale=0.36]{fig/po_CALU_Rn_Rn}}\hfill{}\subfloat[Cond. ALU op. \#123 to Rn]{

\includegraphics[scale=0.36]{fig/po_CALU_123_Rn}}\hfill{}\subfloat[Cond. ALU op. \#123 to PC]{

\includegraphics[scale=0.36]{fig/po_CALU_123_PC}}
\par\end{centering}

\caption{TPG specifications of instruction classes\label{app-fig-Scenarios-of-8}}
\vspace{-6mm}
\end{figure*}


\textbf{ALU operation \#123 to Rn}\quad{}In this class of instructions
one of the operands is a register and the other is a constant which
is given immediately after the instruction opcode (e.g. $\mathit{SUB\ A,\ \#5}$
-- subtraction $A:=A-5$), so called \emph{immediate addressing mode}.
At first, the constant has to be fetched into IR, modelled as $\mathit{PCIU}\rightarrow\mathit{IFU}$.
Then ALU is executed concurrently with another increment of PC: $\mathit{ALU}+\mathit{PCIU'}$
(we use $'$ to distinguish the different occurrences of actions of
the same unit). Finally, it is possible to fetch the next instruction
into IR: $\mathit{IFU'}$. The overall scenario is then $\mathit{PCIU}\rightarrow\mathit{IFU}\rightarrow(\mathit{ALU}+\mathit{PCIU'})\rightarrow\mathit{IFU'}$.

\textbf{ALU operation Rn to PC}\quad{}This class contains operations
for unconditional branching, in which PC register is modified. Branching
can be absolute or relative: $\mathit{MOV\ PC,\ A}$ -- absolute branch
to address stored in register $A$, $PC:=A$; $\mathit{ADD\ PC,\ B}$
-- relative branch to the address $B$ instructions ahead of the current
address, $PC:=PC+B$. The scenario is very simple in this case: $\mathit{ALU}\rightarrow\mathit{IFU}$.

\textbf{ALU operation \#123 to PC}\quad{}Instructions in this class
are similar to those above, with the exception that the branch address
or offset is specified explicitly as a constant. The execution scenario
is composed of : $\mathit{PCIU}\rightarrow\mathit{IFU}$ (to fetch
the constant), followed by an ALU operation, and finally by another
IFU operation, $\mathit{IFU'}$. Hence, the overall scenario is $\mathit{PCIU}\rightarrow\mathit{IFU}\rightarrow\mathit{ALU}\rightarrow\mathit{IFU'}$.

\textbf{Memory access}\quad{}There are two instructions in this class:
$\mathit{MOV\ A,\ [B]}$ and $\mathit{MOV\ [B],\ A}$. They load/save
register $A$ from/to memory location with address stored in register
$B$. Due to the presence of separate program and data memory access
blocks, this memory access can be performed concurrently with the
next instruction fetch: $\mathit{PCIU}\rightarrow\mathit{IFU}+\mathit{MAU}$.

\textbf{Conditional instructions}\quad{}These three classes of instructions
are similar to their unconditional versions above with the difference
that they are performed only if the condition $A<B$ holds. The first
ALU action compares registers $A$ and $B$, setting the ALU flag
$lt$ (less than) according to the result of the comparison. This
flag is then checked by the microcontroller in order to decide on
the further scheduling of actions. 

\textbf{Rn~to~Rn}\quad{}This instruction conditionally performs
an ALU operation with the registers (if the condition does not hold,
the instruction has no effect, except changing the ALU flags). The
operation starts with an ALU operation comparing $A$ with $B$; depending
on the result of this comparison, i.e. the status of the flag $lt$,
the second ALU operation may be performed. This is captured by the
expression $\mathit{ALU}\rightarrow[lt]\mathit{ALU'}$. Concurrently
with this, the next instruction is fetched: $\mathit{PCIU}\rightarrow\mathit{IFU}$.
Hence, the overall scenario is $\mathit{PCIU}\rightarrow\mathit{IFU}+\mathit{ALU}\rightarrow[lt]\mathit{ALU'}$.

\textbf{\#123~to~Rn}\quad{}This instruction conditionally performs
an ALU operation with a register and a constant which is given immediately
after the instruction opcode (if the condition does not hold, the
instruction has no effect, except changing the ALU flags). We consider
the two possible scenarios:
\begin{itemize}
\item $A<B$ holds: First, ALU compares $A$ and $B$ concurrently with
a PC increment; since $A<B$ holds, the ALU sets flag $lt$ and the
constant is fetched to the instruction register: $(\mathit{ALU}+\mathit{PCIU})\rightarrow\mathit{IFU}$.
After that PC has to be incremented again, $\mathit{PCIU'}$, and
ALU performs the operation, $\mathit{ALU'}$. Finally, the next instruction
is fetched (it cannot be fetched concurrently with $\mathit{ALU'}$
as ALU is using the constant in IR): $(\mathit{ALU'}+\mathit{PCIU'})\rightarrow\mathit{IFU'}$.
\item $A<B$ does not hold: First, ALU compares $A$ and $B$ concurrently
with a PC increment; since $A<B$ does not hold, the ALU resets flag
$lt$ and the constant that follows the instruction opcode is skipped
by incrementing the PC: $(\mathit{ALU}+\mathit{PCIU})\rightarrow\mathit{PCIU'}$.
Finally, the next instruction is fetched: $\mathit{IFU'}$.
\end{itemize}
\begin{figure*}
\centering{}\hspace*{\fill}%
\begin{tabular}{|c||c|}
\hline 
{\small Instructions class} & {\small Opcode: $xyz$}\tabularnewline
\hline 
\hline 
{\small ALU Rn to Rn} & {\small 000}\tabularnewline
\hline 
{\small ALU \#123 to Rn} & {\small 110}\tabularnewline
\hline 
{\small ALU Rn to PC} & {\small 101}\tabularnewline
\hline 
{\small ALU \#123 to PC} & {\small 010}\tabularnewline
\hline 
{\small Memory access} & {\small 100}\tabularnewline
\hline 
{\small C/ALU Rn to Rn} & {\small 001}\tabularnewline
\hline 
{\small C/ALU \#123 to Rn} & {\small 111}\tabularnewline
\hline 
{\small C/ALU \#123 to PC} & {\small 011}\tabularnewline
\hline 
\end{tabular}\hspace*{\fill}\raisebox{-5em}[0em]{\includegraphics[scale=0.5]{fig/CPOG_L_3}}\hspace*{\fill}\caption{Optimal 3-bit instruction opcodes and the corresponding TPG specification
of the microcontroller\label{fig:opcodes-and-CG}}
\vspace{-6mm}
\end{figure*}


Hence, the overall scenario is the overlay of the two subscenarios
above prefixed with appropriate conditions (here we denote the predicate
$A<B$ by $lt$): 
\[
\begin{array}{c}
[lt]((\mathit{ALU}+\mathit{PCIU})\!\rightarrow\!\mathit{IFU}\!\rightarrow\!(\mathit{ALU'}+\mathit{PCIU'})\!\rightarrow\!\mathit{IFU'})+\\
+[\overline{lt}]((\mathit{ALU}+\mathit{PCIU})\!\rightarrow\!\mathit{PCIU'}\!\rightarrow\!\mathit{IFU'}).
\end{array}
\]
This expression can be simplified using the rules of TPG-algebra:%
\footnote{This case illustrates the advantage of using the new hierarchical
approach that allows to specify the system as a composition of scenarios
and formally manipulate them in an algebraic fashion. In the previous
paper~\cite{2011_mokhov_iet} the CPOG for this class of instruction
was designed monolithically, and because of this the arc between $\mathit{ALU'}$
and $\mathit{IFU'}$ was missed. Adding this arc not only fixes the
dangerous race between these two blocks, but also leads to a smaller
microcontroller due to the additional similarity between TPGs for
this class of instructions and for the one described below.%
}
\[
(\mathit{ALU}+\mathit{PCIU})\!\rightarrow\![lt]\mathit{IFU}\!\rightarrow\!(\mathit{PCIU'}+[lt]\mathit{ALU'})\!\rightarrow\!\mathit{IFU'}.
\]


\textbf{\#123~to~PC}\quad{}This instruction performs a conditional
branching in which the branch address or offset is specified explicitly
as a constant. We consider the two possible scenarios:
\begin{itemize}
\item $A<B$ holds: First, ALU compares $A$ and $B$ concurrently with
a PC increment; since $A<B$ holds, the ALU sets flag $lt$ and the
constant is fetched to the instruction register: $(\mathit{ALU}+\mathit{PCIU})\rightarrow\mathit{IFU}$.
After that ALU performs the branching operation by modifying PC, $\mathit{ALU'}$.
After PC is changed, the next instruction is fetched, $\mathit{IFU'}$.
\item $A<B$ does not hold: the scenario is exactly the same as in the \textbf{\#123~to~Rn
}case when $A<B$ does not hold.
\end{itemize}
Hence, the overall scenario is the overlay of the two subscenarios
above prefixed with appropriate conditions (here we denote the predicate
$A<B$ by $lt$): 
\[
\begin{array}{c}
[lt]((\mathit{ALU}+\mathit{PCIU})\!\rightarrow\!\mathit{IFU}\!\rightarrow\!\mathit{ALU'}\!\rightarrow\!\mathit{IFU'})+\\
+[\overline{lt}]((\mathit{ALU}+\mathit{PCIU})\!\rightarrow\!\mathit{PCIU'}\!\rightarrow\!\mathit{IFU'}).
\end{array}
\]
This expression can be simplified using the rules of TPG-algebra:
\[
(\mathit{ALU}+\mathit{PCIU})\!\rightarrow\!([\overline{lt}]\mathit{PCIU'}+[lt](\mathit{IFU}\!\rightarrow\!\mathit{ALU'}))\!\rightarrow\!\mathit{IFU'}.
\]


The overall specification of the microcontroller can now be obtained
by prefixing the scenarios with appropriate conditions and overlaying
them. These conditions can be naturally derived from the instruction
opcodes. The opcodes can be either imposed externally or chosen with
the view to optimise the microcontroller. In the latter case, TPG-algebra
and TPGs allow for a formal statement of this optimisation problem
and aid in its solving; in particular, the sizes of the TPG-algebra
expression or TPG are useful measures of microcontroller complexity
(there is a compositional translation from a TPG-algebra expression
into a linear-size circuit). In this paper we do not go into details
how to select the optimal encoding, but see~\cite{2011_mokhov_iet}.
We just note that it is natural to use three bits for opcodes as there
are eight classes of instructions, and give an example of optimal
3-bit encoding in the table in Fig.~\ref{fig:opcodes-and-CG}; the
TPG specification of the corresponding microcontroller is shown in
the right part of this figure (the TPG-algebra expression is not shown
because of its size).
